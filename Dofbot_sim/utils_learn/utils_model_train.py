# âš¡ FK, IK, Sequence Learning è®­ç»ƒæµç¨‹
import time, numpy as np, torch, torch.nn as nn, torch.optim as optim
from pathlib import Path
import pandas as pd
from utils_learn.flexible_networks import FlexibleMLP
import torch
import torch.nn as nn
import torch.distributions as D
import torch.nn.functional as F
from sklearn.model_selection import train_test_split

# ---------- å·¥å…·å‡½æ•° ----------
def select_cols(data_df, names):
    # ä» DataFrame é€‰å–åˆ—å¹¶è¿”å› ndarray
    return data_df[names].values


def split_data(data_df, in_cols, out_cols):
    # ä» DataFrame æ‹†å‡ºè¾“å…¥/è¾“å‡ºï¼Œå¹¶æŒ‰ 8:2 åˆ’åˆ† train/test
    X = select_cols(data_df, in_cols)
    Y = select_cols(data_df, out_cols)
    
    return train_test_split(X, Y, test_size=0.2, random_state=42)

def split_data_analytic(data_df, in_cols, out_cols, dofbot):
    # ä» DataFrame æ‹†å‡ºè¾“å…¥/è¾“å‡ºï¼Œå¹¶æŒ‰ 8:2 åˆ’åˆ† train/test
    X = select_cols(data_df, in_cols)
    Y = select_cols(data_df, out_cols)
    # è§£æè§£
    Y_analytic = analytic_fk(X, dofbot=dofbot)
    # æ®‹å·®
    Y_residual = Y - Y_analytic

    return train_test_split(X, Y_residual, Y_analytic, test_size=0.2, random_state=42)


def compute_fk_loss(y_pred, y_true, w_pos=0.9, w_ori=0.1):
    """
    FK æŸå¤±
    å‚æ•°
    ----
    y_pred : [B, 12]  é¢„æµ‹ä½å§¿ï¼ˆå½’ä¸€åŒ–ï¼‰
    y_true : [B, 12]  çœŸå€¼ä½å§¿ï¼ˆä»…ç›‘æ§ï¼‰
    w_pos  : ä½ç½®è¯¯å·®æƒé‡
    w_ori  : å§¿æ€è¯¯å·®æƒé‡
    è¿”å›
    ----
    loss : æ ‡é‡å¼ é‡
    info : dict
    """
    # 1. ä½ç½®
    loss_pos = F.mse_loss(y_pred[:, :3], y_true[:, :3])

    # 2. æ—‹è½¬çŸ©é˜µè¯¯å·®ï¼ˆFrobenius è·ç¦»ï¼‰
    R_pred = y_pred[:, 3:]  # [B, 9]
    R_true = y_true[:, 3:]  # [B, 9]
    loss_ori = F.mse_loss(R_pred, R_true)
    # 3. åŠ æƒ
    # loss = w_pos * loss_pos + w_ori * loss_ori # åŠ æƒæ€»æŸå¤±
    # 4. ç›‘æ§
    with torch.no_grad():
        mae = torch.mean(torch.abs(y_pred - y_true)).item() # æ•´ä½“ MAE
        rmse = torch.sqrt(torch.mean((y_pred - y_true) ** 2)).item() # æ•´ä½“ RMSE
        pos_error = torch.norm(y_pred[:, :3] - y_true[:, :3], dim=1).mean().item() # ä½ç½®è¯¯å·®
        ori_error = torch.norm(R_pred - R_true, dim=1).mean().item() # å§¿æ€è¯¯å·®

    loss = F.mse_loss(y_pred, y_true) # æ•´ä½“ MSE ä½œä¸ºæŸå¤±
    return loss, {'mae': mae, 'rmse': rmse, 'position_error': pos_error, 'orientation_error': ori_error}


def compute_ik_loss(q_pred, q_true,
                    pose_true=None, fk_ref=None,
                    w_pos=0.9, w_ori=0.1):
    """
    IK æŸå¤±
    å‚æ•°
    ----
    q_pred : [B, 5]  é¢„æµ‹å…³èŠ‚è§’ï¼ˆå½’ä¸€åŒ–ï¼‰
    q_true : [B, 5]  çœŸå€¼å…³èŠ‚è§’ï¼ˆä»…ç›‘æ§ï¼‰
    pose_true:[B, 12] çœŸå€¼æœ«ç«¯çŸ©é˜µ [x,y,z | 9-elements-of-R] ï¼ˆé€šè¿‡è®­ç»ƒå¥½çš„ FK ç½‘ç»œè®¡ç®—ï¼‰
    fk_ref : å†»ç»“çš„ FK ç½‘ç»œï¼Œè¾“å…¥ q è¾“å‡º [B,12]
    w_pos  : ä½ç½®è¯¯å·®æƒé‡
    w_ori  : å§¿æ€è¯¯å·®æƒé‡

    è¿”å›
    ----
    loss : æ ‡é‡å¼ é‡
    info : dict
    """
    # ---- 1. å…³èŠ‚è§’ç›‘æ§ï¼ˆæ— æ¢¯åº¦ï¼‰ ----
    with torch.no_grad():
        joint_mae = torch.mean(torch.abs(q_pred - q_true)).item() # å…³èŠ‚ MAE
        joint_rmse = torch.sqrt(torch.mean((q_pred - q_true) ** 2)).item() # å…³èŠ‚ RMSE

    # ---- 2. æ— çŸ©é˜µç›‘ç£ â†’ é€€åŒ–ä¸ºå…³èŠ‚ MSE ----
    if pose_true is None or fk_ref is None:
        loss = F.mse_loss(q_pred, q_true) # æ—  FK ç›‘ç£ï¼šå…³èŠ‚ MSE ä½œä¸ºæŸå¤±
        info = {'joint_mae': joint_mae, 'joint_rmse': joint_rmse}
        return loss, info

    # ---- 3. FK ç›‘ç£ â†’ æœ«ç«¯çŸ©é˜µæŸå¤± ----
    pred_mat = fk_ref(q_pred)  # [B,12]

    # 3.1 ä½ç½®æŸå¤±
    loss_pos = F.mse_loss(pred_mat[:, :3], pose_true[:, :3])

    # 3.2 æ—‹è½¬çŸ©é˜µæŸå¤±ï¼ˆFrobeniusï¼‰
    loss_ori = F.mse_loss(pred_mat[:, 3:], pose_true[:, 3:])
    # # 3.3 åŠ æƒæ€»æŸå¤±
    # loss = w_pos * loss_pos + w_ori * loss_ori

    loss = F.mse_loss(pred_mat, pose_true) # FK ç›‘ç£ï¼šæœ«ç«¯çŸ©é˜µ MSE ä½œä¸ºæŸå¤±

    # ---- 4. ç›‘æ§æŒ‡æ ‡ ----
    with torch.no_grad():
        pos_err = torch.norm(pred_mat[:, :3] - pose_true[:, :3], dim=1).mean().item() # ä½ç½®è¯¯å·®
        ori_err = torch.norm(pred_mat[:, 3:] - pose_true[:, 3:], dim=1).mean().item() # å§¿æ€è¯¯å·®

    info = {'joint_mae': joint_mae,
            'joint_rmse': joint_rmse,
            'position_error': pos_err,
            'orientation_error': ori_err}

    return loss, info


def plot_training_curves(history: dict, save_path: str):
    # ä¿å­˜è®­ç»ƒ/æµ‹è¯•æ›²çº¿
    import matplotlib.pyplot as plt
    plt.figure(figsize=(6, 4))
    plt.plot(history['train'], label=f"Train {history['metric']}")
    plt.plot(history['test'], label=f"Test  {history['metric']}")
    plt.xlabel('Epoch');
    plt.ylabel(history['metric']);
    plt.title(f"{history['metric']} Curve")
    plt.legend();
    plt.tight_layout();
    plt.savefig(save_path, dpi=300);
    plt.close()
    print(f'ğŸ“ˆ æ›²çº¿å·²ä¿å­˜ â†’ {save_path}')

def analytic_fk(q, dofbot):
    # æ­£è¿åŠ¨å­¦è§£æ
    q = np.array(q)
    if q.shape[1] == 10:  # sin/cos å±•å¼€
        q_angles = np.arctan2(q[:, ::2], q[:, 1::2])  # [B,5]
    elif q.shape[1] == 5:  # ç›´æ¥è§’åº¦
        q_angles = q
    else:
        raise ValueError(f"è¾“å…¥ç»´åº¦é”™è¯¯: q.shape={q.shape}, æœŸæœ›ä¸º [B,5] æˆ– [B,10]")

    # è®¡ç®— FK
    B = q_angles.shape[0]
    pose_list = []

    for i in range(B):
        T = dofbot.fkine(q_angles[i])  # æ­£è¿åŠ¨å­¦è®¡ç®—
        Tm = np.array(T.A)  # å–å‡º4x4çŸ©é˜µ
        xyz = Tm[:3, 3]  # æœ«ç«¯ä½ç½®
        rot = Tm[:3, :3].ravel()  # å±•å¹³æ—‹è½¬çŸ©é˜µ (nx,ny,nz, ox,oy,oz, ax,ay,az)
        pose = np.hstack([xyz, rot])
        pose_list.append(pose)

    # è¿”å› 12 ç»´ï¼šxyz + I9
    pose = np.vstack(pose_list)  # [B, 12]
    return pose

# ---------- å”¯ä¸€å…¥å£ ----------
def train_dofbot_model(data_path,
                       model_type='mlp',  # 'mlp' | 'mdn' | 'lstm'
                       mode='fk',  # 'fk'  | 'ik'
                       in_cols=None,  # list[str] ä»… fk æœ‰æ•ˆ
                       out_cols=None,  # list[str] ä»… ik æœ‰æ•ˆ
                       epochs=1000,
                       lr=1e-3,
                       min_lr=1e-3,
                       num_mixtures=5,
                       hidden_layers=[100, 30],
                       seq_len=10,
                       fk_path=None,
                       fk_hidden_layers=None,
                       w_pos=0.9,
                       w_ori=0.1,
                       use_analytic_fk=False,
                       dofbot=None
                       ):
    # æ•°æ®åŠ è½½ã€æ¨¡å‹æ„é€ ã€è®­ç»ƒã€è¯„ä¼°ã€ä¿å­˜
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    print("device:", device)

    # 0. ç¡®å®šè¾“å…¥/è¾“å‡ºåˆ—å
    data_df = pd.read_csv(data_path)
    if mode == 'fk':
        in_cols = in_cols or ['q1_sin', 'q1_cos', 'q2_sin', 'q2_cos', 'q3_sin', 'q3_cos', 'q4_sin', 'q4_cos', 'q5_sin', 'q5_cos']
        out_cols = out_cols or ['x', 'y', 'z', 'nx', 'ny', 'nz', 'ox', 'oy', 'oz', 'ax', 'ay', 'az']  # é»˜è®¤ xyz+orn
    else:  # ik
        in_cols = in_cols or ['x', 'y', 'z', 'nx', 'ny', 'nz', 'ox', 'oy', 'oz', 'ax', 'ay', 'az']
        out_cols = out_cols or ['q1_sin', 'q1_cos', 'q2_sin', 'q2_cos', 'q3_sin', 'q3_cos', 'q4_sin', 'q4_cos', 'q5_sin', 'q5_cos']
        
        # åŠ è½½å†»ç»“ FK ï¼ˆåŸºäºå·²è®­ç»ƒçš„FKæ¨¡å‹ç›‘ç£è®­ç»ƒï¼‰
        fk_ref = FlexibleMLP(len(out_cols), len(in_cols), hidden_layers=fk_hidden_layers, dropout=0.0,
                             activation='ReLU', block_type='res',
                             num_blocks=1).to(device)
        fk_ref.load_state_dict(torch.load(fk_path, map_location=device, weights_only=True))
        fk_ref.eval()
        for p in fk_ref.parameters():
            p.requires_grad = False

    # 1. å‡†å¤‡æ•°æ®
    if use_analytic_fk and mode == 'fk':
        x_train, x_test, y_train, y_test, y_analytic_train, y_analytic_test = split_data_analytic(data_df, in_cols, out_cols, dofbot)   
        x_train = torch.tensor(x_train, dtype=torch.float32, device=device)
        y_train = torch.tensor(y_train, dtype=torch.float32, device=device)
        y_analytic_train = torch.tensor(y_analytic_train, dtype=torch.float32, device=device)
        x_test = torch.tensor(x_test, dtype=torch.float32, device=device)
        y_test = torch.tensor(y_test, dtype=torch.float32, device=device)
        y_analytic_test = torch.tensor(y_analytic_test, dtype=torch.float32, device=device)
    else:
        x_train, x_test, y_train, y_test = split_data(data_df, in_cols, out_cols)
        x_train = torch.tensor(x_train, dtype=torch.float32, device=device)
        y_train = torch.tensor(y_train, dtype=torch.float32, device=device)
        x_test = torch.tensor(x_test, dtype=torch.float32, device=device)
        y_test = torch.tensor(y_test, dtype=torch.float32, device=device)
    
    # 2. è¾“å‡ºç›®å½•
    timestamp = time.strftime("%Y%m%d_%H%M%S")
    out_dir = Path("results/learn_model") / f"{model_type}_{mode}_{timestamp}"
    out_dir.mkdir(parents=True, exist_ok=True)

    # 3. æ„é€ æ¨¡å‹
    if model_type == 'mlp':
        model = FlexibleMLP(x_train.shape[1], y_train.shape[1], 
                            hidden_layers,
                            dropout=0.0,  # 0.0% dropout
                            activation='ReLU',
                            block_type='res',
                            num_blocks=1).to(device)  # æ¢æ¿€æ´»å‡½æ•°
        opt = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=1e-4) # Adam ä¼˜åŒ–å™¨
        scheduler = torch.optim.lr_scheduler.CosineAnnealingLR( # å­¦ä¹ ç‡è°ƒåº¦å™¨
            opt, T_max=epochs, eta_min=min_lr)  # eta_min æœ€ä½å­¦ä¹ ç‡
        history = {'train': [], 'test': [], 'metric': 'MSE'}
        best_test = np.inf
        patience = 0

        for epoch in range(epochs):
            model.train()
            opt.zero_grad()
            y_pred = model(x_train)
            
            # è®¡ç®—æŸå¤±
            if not use_analytic_fk:
                if mode == 'fk':
                    loss, info = compute_fk_loss(y_pred, y_train, w_pos=w_pos, w_ori=w_ori)
                else:  # ik 
                    loss, info = compute_ik_loss(y_pred, y_train, pose_true=x_train, fk_ref=fk_ref, w_pos=w_pos, w_ori=w_ori)
            else:
                if mode == 'fk':
                    # åŠ ä¸Šè§£æè§£
                    y_pred = y_pred + y_analytic_train
                    loss, info = compute_fk_loss(y_pred, y_train + y_analytic_train, w_pos=w_pos, w_ori=w_ori)
                else:  # ik
                    loss, info = compute_ik_loss(y_pred, y_train, pose_true=x_train, fk_ref=fk_ref, w_pos=w_pos, w_ori=w_ori)
            
            # åå‘ä¼ æ’­
            loss.backward()
            opt.step()
            scheduler.step()  # é€€ç«
            history['train'].append(loss.item())

            # æ¯ epoch è®°å½•æµ‹è¯•
            with torch.no_grad():
                model.eval()
                if not use_analytic_fk:
                    if mode == 'fk':
                        test_loss, test_info = compute_fk_loss(model(x_test), y_test, w_pos=w_pos, w_ori=w_ori)
                    else:  # ik
                        test_loss, test_info = compute_ik_loss(model(x_test), y_test, pose_true=x_test, fk_ref=fk_ref, w_pos=w_pos, w_ori=w_ori)
                else:
                    if mode == 'fk':
                        # åŠ ä¸Šè§£æè§£
                        y_test_pred = model(x_test) + y_analytic_test
                        test_loss, test_info = compute_fk_loss(y_test_pred, y_test + y_analytic_test, w_pos=w_pos, w_ori=w_ori)
                    else:  # ik
                        test_loss, test_info = compute_ik_loss(model(x_test), y_test, pose_true=x_test, fk_ref=fk_ref, w_pos=w_pos, w_ori=w_ori)
            
            history['test'].append(test_loss.item())
            if (epoch + 1) % max(1, epochs // 100) == 0 or epoch == epochs - 1:
                print(
                    f"[{model_type.upper()} {mode.upper()}] Epoch {epoch + 1}/{epochs} | Train: {loss.item():.6f} | Test: {test_loss.item():.6f} | Test info: {test_info}")

            # æå‰åœæ­¢
            if test_loss < best_test:
                best_test = test_loss
                patience = 0
                torch.save(model.state_dict(), out_dir / 'best_model.pt')
            else:
                patience += 1
                if patience >= 50:
                    print(f"Early stop at epoch {epoch + 1}")
                    break

        torch.save(model.state_dict(), out_dir / 'model.pt')

    # 3. ç”»å›¾
    curve_png = out_dir / f"training_curve_{history['metric'].replace(' ', '_')}.png"
    # å»æ‰ nan å†ç”»
    clean = {}
    for k in ['train', 'test']:
        ser = pd.Series(history[k])  # å« nan çš„åºåˆ—
        ser = ser.ffill()  # å‰ä¸€æœ‰æ•ˆå¸§å¡«å……
        clean[k] = ser.values  # è½¬å› ndarray
    clean['metric'] = history['metric']
    plot_training_curves(clean, str(curve_png))

    print(f"âœ… è®­ç»ƒå®Œæˆï¼æ¨¡å‹ä¸æ›²çº¿å·²ä¿å­˜åˆ° â†’ {out_dir}")
    return model, out_dir, str(out_dir / 'best_model.pt')  # â‘  è¿”å› FK æ¨¡å‹è·¯å¾„


if __name__ == "__main__":
    # è®­ç»ƒæ­£é€†è¿åŠ¨å­¦æ¨¡å‹
    fk_model, fk_dir, fk_path = train_dofbot_model(data_path='../dataset/60000/dofbot_fk_60000_norm.csv',
                                                   model_type='mlp', mode='fk',
                                                   fk_out_cols=['x', 'y', 'z', 'roll', 'pitch', 'yaw'],
                                                   epochs=2000, lr=1e-3, hidden_layers=[128, 128, 64])
    ik_model, ik_dir, ik_path = train_dofbot_model(data_path='../dataset/60000/dofbot_fk_60000_norm.csv',
                                                   model_type='mlp', mode='ik',
                                                   ik_in_cols=['x', 'y', 'z', 'roll', 'pitch', 'yaw'],
                                                   epochs=2000, lr=1e-3, hidden_layers=[128, 128, 64], fk_path=fk_path,
                                                   fk_hidden_layers=[128, 128, 64])
    # # è®­ç»ƒæ­£é€†è¿åŠ¨å­¦æ¨¡å‹
    # fk_model, fk_dir, fk_path = train_dofbot_model(data_path='dataset/60000/dofbot_fk_60000_norm.csv',
    #                                                model_type='mlp', mode='fk',
    #                                                fk_out_cols=['x', 'y', 'z'],
    #                                                epochs=2000, lr=1e-3, hidden_layers=[128, 128, 64])
    # ik_model, ik_dir, ik_path = train_dofbot_model(data_path='dataset/60000/dofbot_fk_60000_norm.csv',
    #                                                model_type='mlp', mode='ik',
    #                                                ik_in_cols=['x', 'y', 'z'],
    #                                                epochs=2000, lr=1e-3, hidden_layers=[128, 128, 64], fk_path=fk_path)
